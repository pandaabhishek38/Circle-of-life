# Circle-of-life

# Problem Statement

The project consists of a graph of 50 nodes connected by edges. The graph can contain three entities - the predator, the prey, and the agent. These entities can move from node to node along the edges of the given graph by following their respective rules to move around. The predator tries to catch the agent while the agent is pursuing the prey. The agent wins if it catches the prey and it loses if it's caught by the predator. The game is disbanded when the max step count exceeds and neither the predator, nor the agent could catch its target. The aim of this project is to pursue the prey while escaping from the predator.

# Implementation

## The Environment:

We generated a graph using Python dictionary consisting of 50 nodes, numbered 0 to 49, connected in a large circle. The agent, prey and the predator move along the edges connecting these nodes. Additional edges were added between the nodes to increase the connectivity of the graph. We first picked a node of degree less than 3. Then we added an edge between it and a node within 5 steps further or behind in the primary loop. The procedure was repeated until no more edges could be added. This sets up the environment of our project.

The playing ground for this project, that is, the graph is generated by calling functions of the Graph class in the submitted code. Entry point of graph generation starts from the create_graph() function. The create_graph function creates a Python dictionary and immediately creates a two-way path from a particular node to both its adjacent nodes. After this step, every node in the graph will have a degree 2.

For adding the third path, the create_graph() calls the add_edge_3() method of the same class. The add_edge_3() function in turn calls another function edge_available() of the same class. The edge_available() function checks all the left and right side 5 neighbouring nodes of the current node, and returns a list of nodes in that span that have a degree less than 3 back to add_edge_3() function. The add_edge_3() function then randomly picks a node from this returned list and creates a new edge. The following process is repeated for every node until no possible edges can be added.

For any graph generated using the above method, all the nodes will first be connected to their adjacent nodes. Thusj, at this point, every node will have a degree 2. Now, for ease, lets say that every node is then connected to a node on its left side that is 2-edges away from the current node. In doing so, if we keep doing this for all the nodes, then eventually the graph generation algorithm will circle back to the node just on the right side of the initial node. At this point, the algorithm will create an edge between the initial node’s left and the right nodes and there won’t be nay more nodes available with degree less than 3. Now, the point to notice is that when we create an edge between any 2 nodes, the degree of not just the current node is increased, but the other nodes degree is increased by 1 as well. So, after connecting every node to its adjacent node, we will only have 50 orders to update, where adding a single edge between two nodes increases the total order count of the graph by 2. Hence, we will only be able to add 25 edges at max.

## The Predator, the prey and the agent:

As mentioned above, our environment consists of three entities - the predator, the prey and the agent. The agent pursues the prey while being pursued by the predator. They all can move along the edges connecting the nodes. If the agent and the prey occupy the same node at any point of time, the agent wins as it has caught the prey. Whereas if the predator and the agent occupy the same node at any point of time, the agent loses as it has been caught by the predator. There is no issue if the predator and the agent occupy the same node. The three entities move in rounds. The agent makes the first move, followed by the prey and then the predator at last. The three players follow the following rules:

*The Prey:* When its the prey’s turn to make a move, it will select any one of its current neighbors or its current cell, uniformly at random. So if at any point, the prey has 3 neighbors. The probability of the prey picking any 1 of them is 1⁄4 and the probability of not leaving its current node is also 1⁄4. The prey will continue this regardless of the motions of the other two players, until the game concludes.

*The Predator:* When its the predator’s turn to make a move, it will look to move closer to the agent. So it will take a look at each of its neighboring nodes and calculate the distance of each node to the node occupied by the agent at that moment. The predator shall then pick the node with the shortest distance to the agent and then move to that node. In case multiple neighboring nodes have the same shortest distance to the agent, it will select uniformly at random. This behavior of the predator is applicable only to agents 1, 2, 3, and 4. However, for agents 5 and up, the predator acts as a distracted predator. In this, the predator either moves following the shortest path to the agent with a probability of 60%, or it randomly moves to any of its neighbors with a. probability of 40%. This setting is called the distracted predator as is applicable to agents 5, 6, 7, 8, 7_b, 8_b.

*The Agent:* The agent will make its move based on the specific strategy it is following. The agents are implemented in 4 different types of information settings. In the first type, the agent knows the exact location of the predator and the prey at every move. In the second one, the agent will know the location of the predator at each move, but it may not know the location of the prey. In the third information setting, the agent will have information on the location of the prey, but it may not know the exact location of the predator. The final setting is the combination of the previous two settings. The agent does not necessarily know the exact location of both the predator and the prey. In each setting, we have developed different strategies to achieve our aim.

In the code, each agent has its own class. All the agent’s classes except agents 1 and 2 have the below two methods:

1) proceed(): Entry point for simulating the entire game
   
2) survey_node(): Surveys the given node to check whether it has prey or the predator in it. We have implemented in differently in separate Agent classes as survey node functions differently for different agents. For agents 1 and 2, there is no need for these as those agents always know the location of both the prey and the predator. For agents 3 and 4, they survey_node() function checks whether a particular node has the prey in it or not. For agents 5 and 6, it checks for the predator. For agents 7, 8, 7_b and 8_b, it checks if either are present or not.

# The Complete Information Setting

## Overview:

In this strategy, the agent will examine all its neighbors and choose the best node to move to, based on the undermentioned rules. In this setting, the agent always knows the location of both the prey and the predator. So, there was no need to calculate probability in this case. We used Breadth-First search algorithm to find the shortest path between two nodes. Using this shortest path, the agent decides its next node and the predator decides its next node from this shortest path as well. The agent in this setting chooses its next node as per the below rules in the same order:

• Neighbors that are closer to the prey and farther from the predator

• Neighbors that are closer to the prey and not closer to the predator

• Neighbors that are not farther from the prey and farther from the predator

• Neighbors that are not farther from the prey and not closer to the predator

• Neighbors that are farther from the predator

• Neighbors that are not closer to the predator

• Remain in the same node

Ties are broken randomly.

# Agent1

## Implementation:

The Proceed() function inside the Agent1() class runs the game’s simulation for the pre-decided step_count. If the game ends in this constraint, it returns 1 if the agent wins, and 2 if the predator catches the agent. If the step count threshold is crossed, the game ends and the system returns 0 to indicade that the game was disbanded. The agent 1, at any point of time, always knows the predator’s and prey’s locations. So, the agent first calculates the distance from its current position to the predator and the prey using Breadth-First search by calling the calc_path() function of the Graph() class. The agent also computes the shortest path to the predator and prey from all of its neighboring nodes. It then creates 2 lists(1 for predator and 1 for prey) that contain these paths and calls the decide_node() function of the agents_common() class.

It is in the agents_common.decide_node() function that the above stated rules are coded. The code runs starting with checking the condition 1, then condition 2, and so on. If at any point, any of the above condition is satisfied, then the code picks that node and returns its value. So, if condition 1 is satisfied and a node is identified as the next node, the rest of the conditions will not be checked. In this way, the performance of the code gets a little better. Also in this way, rule number 1 is given the highest priority, then rule number 2, then rule number 3, and so on.

![New Note](https://github.com/pandaabhishek38/Circle-of-life/assets/56110423/5d0844d1-c645-4b0f-b7b0-c228e4f50b21)

Looking at Fig. 1, we can see that the agent 1 start with a survival probability of around 86% for 50 iterations. As per Fig. 2, win count for agent 1 is around 2600 games, lose count is around 374 at max step-count 50. However, the agent 1’s game also disbands/hangs a total of 26 times. So, we keep on increasing the step count limit until all the games get completed within the given limit. We find that for a 300 step count limit the agent 1 is able to complete all the 3000 games without any hang. So, just take we actual survivability of the agent, we consider the survivability of the agent 1 at step count 300, which is 89.5%.

# Agent 2

## Implementation:

The proceed() function inside the Agent2() class runs the game’s simulation for the pre-decided step_count. The agent 2 returns the same values that agent 1 returns to indicate how the game went. The agent 2, at any point of time, always knows the predator’s and prey’s locations and it also uses the same calc_path() function of the Graph() class to calculate the shortest path from node A to node B. The agent computes the shortest path to the predator and prey from all of its neighboring nodes. It then also creates 2 lists (1 for predator and 1 for prey) that contain these paths. However, instead of calling the decide_node() function of the agents_common() class, the agent 2 (and all other even numbered agents) call the decide_node_even() function of the agents_common() class to decide the next node.

It is in the agents_common.decide_node_even() function first checks the agent’s current distance from the predator. If that distance is less than 5, then tries to move away from the predator. If the agent is current 5 or more moves away from the predator, then the decide_node_even() function calls the decide_node() function of the same class to select the next node as per the rules that are written above in the overview section of The Complete Information Setting section.

![New Note](https://github.com/pandaabhishek38/Circle-of-life/assets/56110423/049a3b19-f610-4272-8408-75fa0a6ce633)

Looking at Fig. 3, we can see that the agent 2 start with a survival probability of around 91% for 50 iterations. As per Fig. 4, win count for agent 2 is around 2726 games, lose count is around 176 at max step-count 50. However, the agent 2’s game also disbands/hangs a total of 98 times. So, we keep on increasing the step count limit until all the games get completed within the given limit. We find that for a 300 step count limit the agent 2 is able to complete all the 3000 games without any hang. So, just take we actual survivability of the agent, we consider the survivability of the agent 2 at step count 3000, which is 94.83%, which is more than better than agent 1’s survivability. Agent 2’ win count in 3000 games for this max step count is 2845, lose count is 155, and hang count is 0.

# The Partial Prey Information Setting

In this type of setting, the agent knows the exact location of the predator at every move but does not know the location of the prey. In each move, the agent can pick a node with the maximum probability of it containing the prey, and checks whether the prey is there or not. The agent gains more information about the prey isn’t every time it enters a node and the prey isn’t there. The agent needs to keep track of a belief state for where the prey is, a collection of probabilities for each node that prey is there. With each move, the agent learns something about the prey, and hence the probabilities must be updated every time we learn something new. And every time the prey is known to move, these probabilities must be updated. Two strategies have been developed in this setting.

# Agent 3

## Implementation:

The Agent 3 does now know the location of the prey, so it computes the probability of all the nodes if that node containing the prey by calling the update_prey_prob_presurvey() function of the agents_common class, however it does know the exact location of the predator throughout the game. The agent then survey’s that node with maximum probability of containing the prey (breaking ties at random). If the prey is present in that location, it updates the probability of that node containing the prey to 1, and probability of all other nodes to 0, and then proceeds towards that node. If the agent could not find the prey in the survey node, it then sets the probability of the survey node to zero and re-calculates the probability of all the other nodes by calling the update_prey_prob_postsurvey() function of the agents_common class based on the result of the survey action and the previous probability distribution. After this, the agent selects the node with the maximum probability of containing the prey and starts moving to that node by following the rules mentioned in the overview section of The Complete information setting by calling the decide_node() function of the agents_common class.

The basic structure of agent 3 for every step/move looks something as follows:

1. Calculate the probability of the prey
 
2. Survey the node with maximum probability
 
3. Update the probability distribution of the entire graph based on the survey results
 
4. Pick the node with the maximum probability and move towards that node
 
5. Check if any condition to end the game is met or not
 
6. The prey moves
 
7. Check if any condition to end the game is met or not
 
8. The predator moves by following the shortest path to the agent
 
9. Check if any condition to end the game is met or not
 
10. Increment the step_count and move to the next iteration

The probability distribution for agent 3 is calculated in the following manner:

Probability calculation before survey:
For the first move of the game, assign each the a probability of 1/(no. of total nodes-1), that is, each node in our case will have a probability of 1/(50-1) = 1/49 for the prey.

For all other steps, the update_prey_prob_presurvey uses the below formula to calculate the prey probability before survey:

P(x) = ∑ (P(in x now, was in i)) for i in 0...49
= ∑ (P(was in i) . P(in x | was in i)) for i in 0...49

Where, x is the node whose probability if getting calculated that is taken from the outer loop inside the function

i is all the nodes nodes that are taken one-by-on in the internal loop

Based on the above, the agents picks a node with the maximum probability in the agent’s proceed() function to survey. If there are multiple nodes that have the same max probability, then it chooses any node from those nodes in a random manner for survey.

After survey, the agent again updates the probability distribution by calling the update_prey_prob_postsurvey() function of the agents_common class. The update_prey_prob_postsurvey() function calculates the probability by using the below logic, using the present probability distribution and the survey result.

If survey returns True, that is, the prey is present at the surveyed node: 

P(survey_node) = 1

P(i) = 0 for i in 0..49 and i != survey_node

If the survey returns False, that is, the prey is not present in the node that was surveyed, then the probability distribution is updated using the below formula:

P(survey_node) = 0

P(x) = P(x) . P(not in survery_node|in x)/(∑ P(i) . P(not in survery_node | in i)) for i and x in 0..49

Where, P(not in survery_node|in x) for survey node is set to 0, for all others, it is 1 

P(not in survery_node | in i) for survey node is set to 0, for all others, it is 1

P(i) and P(x) are the probability values that were previously calculated by calling the update_prey_prob_presurvey() function.

Based on the above action, the agent 3 then picks the node with the maximum probability value, generates the list containing distance from the current and neighboring nodes to the predator and chosen prey node and calls the decide_node () function to pick the next node and proceed.

The survey_node() function of Agent 4 class checks whether the prey is present in the survey node or not.

![New Note](https://github.com/pandaabhishek38/Circle-of-life/assets/56110423/39980339-91ff-4076-88a7-38037b07bc6e)

Looking at Fig. 5, we can see that the agent 3 start with a survival probability of around 65% for 50 iterations. As per Fig. 6, win count for agent 3 is around 1951 games, lose count is around 794 at max step-count 50. However, the agent 3’s game also disbands/hangs a total of 255 times. So, we keep on increasing the step count limit until all the games get completed within the given limit. We find that for a 300 step count limit the agent 3 is able to complete all the 3000 games without any hang. So, just take we actual survivability of the agent, we consider the survivability of the agent 3 at step count 3000, which is 72.57%. Agent 3’ win count in 3000 games for this max step count is 2177, lose count is 823, and hang count is 0.

We also calculate the percentage of times the agent 3 knows the prey’s actual location. It came out to be 2.941313626% only! The agent starts without knowing the location of the prey and assigns each node except its current node, a probability of 1/49 (0.0204...), which is pretty low! So, when the agent does not really know the prey’s location, it keeps on surveying different nodes and updating the probability. However, at this same time, the prey also keeps moving. So, a node that has been surveyed before may now occupy the prey. The chance of the agent surveying the prey’s location is pretty low and the game becomes more of not chasing the prey, but staying away from the predator and trying our luck to find the prey. Due to this reason, the agent does not really know the location of the prey a lot of times. Hence, the above percentage value seems low.

# Agent 4

## Implementation:

The Agent 4 can be considered as an increment over agent 3. The basic approach that agent 4 is very similar to the agent 3, with some modifications that increase its survivability percentage.

Just for now, let’s consider a Boolean flag prey_next_calculated = False. This value is set outside the loop of the proceed() function, so it is ONLY initialized once. After this, the code in the loop keeps using the flag to consider whether or not to re-calculate the probability of the prey.

The basic structure of agent 4 for every step/move looks something as follows:

1. Calculate the probability of the prey if prey_next_calculated = False. If prey_next_calculated = True, then do not calculate the probability now and just update the Flag value to False. In the first iteration, it’s value will be False, as initialized.

2. Survey the node with maximum probability
  
3. Update the probability distribution of the entire graph based on the survey results.
 
4. Pick the node with the maximum probability
 
5. Based on the node picked in step 4, if the agent is currently right next to the prey, then the agent sets the node that was found in step 4 as possible prey position and goes to step 9

6. Call the update_prey_prob_presurvey() function that was used before surveying to update the probability distribution of the prey one more time and update the flag prey_next_calculated to True
 
7. Pick the node from the probability distribution with the maximum probability value. This node will be the node that the agent will be in after it moves from its current position

8. Call the decide_node_even() function with prey_position that was calculated based on the probabilities, and move towards that node

9. Check if any condition to end the game is met or not
  
10. The prey moves
 
11. Check if any condition to end the game is met or not
 
12. The predator moves by following the shortest path to the agent
 
13. Check if any condition to end the game is met or not
  
14. Increment the step_count and move to the next iteration

By calling the update_prey_prob_presurvey() once more time after survey and after calling the update_prey_prob_postsurvey(), we can calculate the prey’s next possible position and move towards that node so as to reach that node in a quicker manner. Once we update the probabilities after survey, we do not require the probabilities to be calculated once again before survey of the enxt iteration. So, that is when the Boolean flag prey_next_calculated will be used to know if the probabilities were updated one more time in the previous iteration of the loop.

To calculate the pre-survey and post-survey probabilities, the agent 4 calls the same functions that Agent 3 calls. So the agent 4 also uses the same formulas that are mentioned in the implemention of agent 3.

![New Note](https://github.com/pandaabhishek38/Circle-of-life/assets/56110423/f0fe3b21-0c0c-493f-812d-79527cf47d04)

Looking at Fig. 7, we can see that the agent 4 start with a survival probability of around 73% for 50 iterations. As per Fig. 8, win count for agent 4 is 2196 games, lose count is 397 at max step-count 50. However, the agent 3’s game also disbands/hangs a total of 407 times, which is greater than it’s lose count! So, we keep on increasing the step count limit until all the games get completed within the given limit. We find that for a 300 step count limit the agent 4 is able to complete all the 3000 games without any hang. So, just to take the actual survivability of the agent, we consider the survivability of the agent 4 at step count 300, which is 83.87%, which is better than Agent 3! Agent 4’s win count in 3000 games for this max step count is 2516, lose count is 484, and hang count is 0.

We also calculate the percentage of times the agent 4 knows the prey’s actual location. It came out to be 0.996665555% only! We believe the reason why this value is so low is provided in the analysis given for this value for agent 3 (at the end-side of agent 3’s section)
